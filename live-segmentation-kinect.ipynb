{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Object Segmentation with OpenVINO\n",
    "\n",
    "This notebook demonstrates live instance segmentation with OpenVINO, using the [EfficientNet-B1](https://docs.openvino.ai/latest/omz_models_model_instance_segmentation_person_0007.html#training-pipeline) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). Final part of this notebook shows live inference results from a webcam. Additionally, you can also upload a video file.\n",
    "\n",
    "> **NOTE**: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server, the webcam will not work. However, you can still do inference on a video.\n",
    "\n",
    "> **NOTE**: This notebook adapts the [tutorial](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/401-object-detection-webcam) from object detection to instance segmentation. This means quite a lot is modified from the original notebook, since the model isn't the same, the pre and postprocessing of each frame need to be adapted and implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from openvino.runtime import Core\n",
    "\n",
    "from freenect2 import Device, FrameType\n",
    "\n",
    "# sys.path.append(\"./openvino_notebooks/notebooks/utils/\")\n",
    "# import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "### Download the Model\n",
    "\n",
    "Use `omz_downloader`, which is a command-line tool from the `openvino-dev` package. It automatically creates a directory structure and downloads the selected model. This step is skipped if the model is already downloaded. The selected model comes from the public directory, which means it must be converted into OpenVINO Intermediate Representation (OpenVINO IR).\n",
    "\n",
    "If you want to download another model (`ssdlite_mobilenet_v2`, `ssd_mobilenet_v1_coco`, `ssd_mobilenet_v2_coco`, `ssd_resnet50_v1_fpn_coco`, `ssd_mobilenet_v1_fpn_coco`) , replace the name of the model in the code below. \n",
    "\n",
    "> **NOTE**: Using a model outside the list can require different pre- and post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = \"models/model-segmentation\"\n",
    "\n",
    "model_segmentation = \"instance-segmentation-person-0007\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output path for the conversion.\n",
    "converted_model_path = f\"models/model-segmentation/intel/{model_segmentation}/FP16/{model_segmentation}.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model\n",
    "\n",
    "Downloaded models are located in a fixed structure, which indicates a vendor (intel or public), the name of the model and a precision.\n",
    "\n",
    "Only a few lines of code are required to run the model. First, initialize OpenVINO Runtime. Then, read the network architecture and model weights from the `.bin` and `.xml` files to compile for the desired device. If you choose `GPU` you need to wait for a while, as the startup time is much longer than in the case of `CPU`.\n",
    "\n",
    "There is a possibility to allow OpenVINO to decide which hardware offers the best performance. In that case, just use `AUTO`. Remember that for most cases the best hardware is `GPU` (better performance, but longer startup time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime.\n",
    "ie_core = Core()\n",
    "# Read the network and corresponding weights from a file.\n",
    "model = ie_core.read_model(model=converted_model_path)\n",
    "# Compile the model for CPU (you can choose manually CPU, GPU, MYRIAD etc.)\n",
    "# or let the engine choose the best available device (AUTO).\n",
    "compiled_model = ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "# Get the input and output nodes.\n",
    "input_layer = compiled_model.input(0)\n",
    "output_boxes = compiled_model.output(0)\n",
    "\n",
    "# Get the input size.\n",
    "height, width = list(input_layer.shape)[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ConstOutput: names[boxes] shape[?,5] type: f32>,\n",
       " <ConstOutput: names[labels, 1973] shape[?] type: i64>,\n",
       " <ConstOutput: names[masks] shape[?,224,224] type: f32>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Processing\n",
    "\n",
    "### Process Results\n",
    "\n",
    "Functions adapted from:\n",
    "- https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/common/python/visualizers/instance_segmentation.py\n",
    "- https://github.com/openvinotoolkit/open_model_zoo/blob/5a3376f908abc0a1cae8838af6af89931644321d/demos/instance_segmentation_demo/python/instance_segmentation_demo.py#L31\n",
    "- https://github.com/openvinotoolkit/open_model_zoo/blob/5a3376f908abc0a1cae8838af6af89931644321d/demos/common/python/openvino/model_zoo/model_api/models/instance_segmentation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import colorsys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ColorPalette:\n",
    "    def __init__(self, n, rng=None):\n",
    "        if n == 0:\n",
    "            raise ValueError('ColorPalette accepts only the positive number of colors')\n",
    "        if rng is None:\n",
    "            rng = random.Random(0xACE)  # nosec - disable B311:random check\n",
    "\n",
    "        candidates_num = 100\n",
    "        hsv_colors = [(1.0, 1.0, 1.0)]\n",
    "        for _ in range(1, n):\n",
    "            colors_candidates = [(rng.random(), rng.uniform(0.8, 1.0), rng.uniform(0.5, 1.0))\n",
    "                                 for _ in range(candidates_num)]\n",
    "            min_distances = [self.min_distance(hsv_colors, c) for c in colors_candidates]\n",
    "            arg_max = np.argmax(min_distances)\n",
    "            hsv_colors.append(colors_candidates[arg_max])\n",
    "\n",
    "        self.palette = [self.hsv2rgb(*hsv) for hsv in hsv_colors]\n",
    "\n",
    "    @staticmethod\n",
    "    def dist(c1, c2):\n",
    "        dh = min(abs(c1[0] - c2[0]), 1 - abs(c1[0] - c2[0])) * 2\n",
    "        ds = abs(c1[1] - c2[1])\n",
    "        dv = abs(c1[2] - c2[2])\n",
    "        return dh * dh + ds * ds + dv * dv\n",
    "\n",
    "    @classmethod\n",
    "    def min_distance(cls, colors_set, color_candidate):\n",
    "        distances = [cls.dist(o, color_candidate) for o in colors_set]\n",
    "        return np.min(distances)\n",
    "\n",
    "    @staticmethod\n",
    "    def hsv2rgb(h, s, v):\n",
    "        return tuple(round(c * 255) for c in colorsys.hsv_to_rgb(h, s, v))\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        return self.palette[n % len(self.palette)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceSegmentationVisualizer:\n",
    "    def __init__(self, labels=None, show_boxes=False, show_scores=False):\n",
    "        colors_num = len(labels) if labels else 80\n",
    "        self.labels = labels\n",
    "        self.palette = ColorPalette(colors_num)\n",
    "        self.show_boxes = show_boxes\n",
    "        self.show_scores = show_scores\n",
    "\n",
    "    def __call__(self, image, boxes, classes, scores, masks=None, dist=None, ids=None, texts=None):\n",
    "        result = image.copy()\n",
    "\n",
    "        if masks is not None:\n",
    "            result = self.overlay_masks(result, masks, ids)\n",
    "        if self.show_boxes:\n",
    "            result = self.overlay_boxes(result, boxes, classes)\n",
    "\n",
    "        result = self.overlay_labels(result, boxes, classes, scores, dist, texts)\n",
    "        return result\n",
    "\n",
    "    def overlay_masks(self, image, masks, ids=None):\n",
    "        segments_image = image.copy()\n",
    "        aggregated_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        aggregated_colored_mask = np.zeros(image.shape, dtype=np.uint8)\n",
    "        all_contours = []\n",
    "\n",
    "        for i, mask in enumerate(masks):\n",
    "            mask = mask.astype(np.uint8)\n",
    "            contours = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "            if contours:\n",
    "                all_contours.append(contours[0])\n",
    "\n",
    "            mask_color = self.palette[i if ids is None else ids[i]]\n",
    "            cv2.bitwise_or(aggregated_mask, mask, dst=aggregated_mask)\n",
    "            cv2.bitwise_or(aggregated_colored_mask, mask_color, dst=aggregated_colored_mask, mask=mask)\n",
    "\n",
    "        # Fill the area occupied by all instances with a colored instances mask image\n",
    "        cv2.bitwise_and(segments_image, (0, 0, 0), dst=segments_image, mask=aggregated_mask)\n",
    "        cv2.bitwise_or(segments_image, aggregated_colored_mask, dst=segments_image, mask=aggregated_mask)\n",
    "\n",
    "        cv2.addWeighted(image, 0.5, segments_image, 0.5, 0, dst=image)\n",
    "        cv2.drawContours(image, all_contours, -1, (0, 0, 0))\n",
    "        return image\n",
    "\n",
    "    def overlay_boxes(self, image, boxes, classes):\n",
    "        for box, class_id in zip(boxes, classes):\n",
    "            color = self.palette[class_id]\n",
    "            box = box.astype(int)\n",
    "            top_left, bottom_right = box[:2], box[2:]\n",
    "            image = cv2.rectangle(image, top_left, bottom_right, color, 2)\n",
    "        return image\n",
    "\n",
    "    def overlay_labels(self, image, boxes, classes, scores, dist=None, texts=None):\n",
    "        if texts:\n",
    "            labels = texts\n",
    "        elif self.labels:\n",
    "            labels = (self.labels[class_id] for class_id in classes)\n",
    "        else:\n",
    "            raise RuntimeError('InstanceSegmentationVisualizer must contain either labels or texts to display')\n",
    "        \n",
    "        if len(dist) >= 1:\n",
    "            template = '{}: {:.2f}, {}:{:.3f}' if self.show_scores else '{}'\n",
    "        else:\n",
    "            template = '{}: {:.2f}' if self.show_scores else '{}'\n",
    "        \n",
    "        count = 0\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            if len(dist) >= 1:\n",
    "                text = template.format(label, score, \"distance\", dist[count])\n",
    "                count += 1\n",
    "            else:\n",
    "                text = template.format(label, score)\n",
    "            textsize = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "            position = ((box[:2] + box[2:] - textsize) / 2).astype(np.int32)\n",
    "            cv2.putText(image, text, position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "classes = [\n",
    "    \"person\", \"background\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\",\n",
    "    \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"street sign\", \"stop sign\",\n",
    "    \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
    "    \"bear\", \"zebra\", \"giraffe\", \"hat\", \"backpack\", \"umbrella\", \"shoe\", \"eye glasses\",\n",
    "    \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n",
    "    \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\",\n",
    "    \"plate\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
    "    \"couch\", \"potted plant\", \"bed\", \"mirror\", \"dining table\", \"window\", \"desk\", \"toilet\",\n",
    "    \"door\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\",\n",
    "    \"toaster\", \"sink\", \"refrigerator\", \"blender\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "    \"teddy bear\", \"hair drier\", \"toothbrush\", \"hair brush\"\n",
    "]\n",
    "\n",
    "def _expand_box(box, scale):\n",
    "        w_half = (box[2] - box[0]) * .5\n",
    "        h_half = (box[3] - box[1]) * .5\n",
    "        x_c = (box[2] + box[0]) * .5\n",
    "        y_c = (box[3] + box[1]) * .5\n",
    "        w_half *= scale\n",
    "        h_half *= scale\n",
    "        box_exp = np.zeros(box.shape)\n",
    "        box_exp[0] = x_c - w_half\n",
    "        box_exp[2] = x_c + w_half\n",
    "        box_exp[1] = y_c - h_half\n",
    "        box_exp[3] = y_c + h_half\n",
    "        return box_exp\n",
    "    \n",
    "def postprocess_mask(box, raw_cls_mask, im_h, im_w):\n",
    "    # Add zero border to prevent upsampling artifacts on segment borders.\n",
    "    raw_cls_mask = np.pad(raw_cls_mask, ((1, 1), (1, 1)), 'constant', constant_values=0)\n",
    "    extended_box = _expand_box(box, raw_cls_mask.shape[0] / (raw_cls_mask.shape[0] - 2.0)).astype(int)\n",
    "    w, h = np.maximum(extended_box[2:] - extended_box[:2] + 1, 1)\n",
    "    x0, y0 = np.clip(extended_box[:2], a_min=0, a_max=[im_w, im_h])\n",
    "    x1, y1 = np.clip(extended_box[2:] + 1, a_min=0, a_max=[im_w, im_h])\n",
    "\n",
    "    raw_cls_mask = cv2.resize(raw_cls_mask.astype(np.float32), (w, h)) > 0.5\n",
    "    mask = raw_cls_mask.astype(np.uint8)\n",
    "    # Put an object mask in an image mask.\n",
    "    im_mask = np.zeros((im_h, im_w), dtype=np.uint8)\n",
    "    im_mask[y0:y1, x0:x1] = mask[(y0 - extended_box[1]):(y1 - extended_box[1]),\n",
    "                                (x0 - extended_box[0]):(x1 - extended_box[0])]\n",
    "    return im_mask\n",
    "\n",
    "def process_results(frame, input_img, results, thresh=0.6):\n",
    "    # The size of the original frame.\n",
    "    h, w = frame.shape[:2]\n",
    "    # size of input image\n",
    "    im_h, im_w = input_img.shape[2:]\n",
    "    #scales\n",
    "    scale_x = im_w / w\n",
    "    scale_y = im_h / h\n",
    "    \n",
    "    # Extract results from list\n",
    "    results_boxes = results[0]\n",
    "    results_labels = results[1]\n",
    "    results_masks = results[2]\n",
    "    #keep only objects with confidence score > 0.2\n",
    "    mask = results_boxes[:, -1] > 0.2\n",
    "    results_boxes = results_boxes[mask]\n",
    "    results_labels = results_labels[mask]\n",
    "    results_masks = results_masks[mask]\n",
    "    \n",
    "    boxes = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    masks = []\n",
    "    \n",
    "    for i, (top_left_x, top_left_y, bottom_right_x, bottom_right_y, score) in enumerate(results_boxes):\n",
    "        box = np.array([top_left_x/scale_x,\n",
    "                        top_left_y/scale_y,\n",
    "                        bottom_right_x/scale_x,\n",
    "                        bottom_right_y/scale_y]).astype(int)\n",
    "        boxes.append(box)\n",
    "        labels.append(int(results_labels[i]))\n",
    "        scores.append(float(score))\n",
    "        masks.append(postprocess_mask(box, results_masks[i], h, w))\n",
    "\n",
    "    return labels, scores, boxes, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Function\n",
    "\n",
    "Run object detection on the specified source. Either a webcam or a video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_to_humans(frame, masks):\n",
    "    dist_list = []\n",
    "    for mask in masks:\n",
    "        dist_list.append(1)\n",
    "    return dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Main processing function to run object detection.\n",
    "def run_object_detection(source=0, flip=False, use_popup=False, skip_first_frames=0):\n",
    "    device = None\n",
    "    video_frames = []\n",
    "    masks = None\n",
    "    dist = []\n",
    "    \n",
    "    try:\n",
    "        device = Device()\n",
    "        device.start()        \n",
    "        \n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        visualizer = InstanceSegmentationVisualizer(classes, True, True)\n",
    "        \n",
    "        # init count depth\n",
    "        t0 = time.time()\n",
    "        while True:            \n",
    "            # Grab the frame.\n",
    "            type_, frame_ = device.get_next_frame()\n",
    "            if frame_ is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "                \n",
    "            if type_ == FrameType.Color:\n",
    "                frame = frame_.to_array().astype(np.uint8)[:,:,0:3]\n",
    "\n",
    "                # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "                scale = 1280 / max(frame.shape)\n",
    "                if scale < 1:\n",
    "                    frame = cv2.resize(\n",
    "                        src=frame,\n",
    "                        dsize=None,\n",
    "                        fx=scale,\n",
    "                        fy=scale,\n",
    "                        interpolation=cv2.INTER_AREA,\n",
    "                    )\n",
    "                    \n",
    "                cv2.resize(src=frame, dsize=(640, 480), interpolation=cv2.INTER_AREA, dst=frame)\n",
    "\n",
    "                # Resize the image and change dims to fit neural network input.\n",
    "                input_img = cv2.resize(\n",
    "                    src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA\n",
    "                )\n",
    "                # Create a batch of images (size = 1).\n",
    "                input_img = input_img[np.newaxis, ...]\n",
    "                input_img = np.moveaxis(input_img, -1, 1)\n",
    "\n",
    "                # Measure processing time.\n",
    "\n",
    "                start_time = time.time()\n",
    "                # Get the results.\n",
    "                results = list(compiled_model([input_img]).values())\n",
    "                stop_time = time.time()\n",
    "                # Get network outputs\n",
    "                labels, scores, boxes, masks = process_results(frame=frame, input_img=input_img, results=results)\n",
    "                # visualize\n",
    "                frame = visualizer(frame, boxes, labels, scores, masks, dist, None, None)\n",
    "\n",
    "                processing_times.append(stop_time - start_time)\n",
    "                # Use processing times from last 200 frames.\n",
    "                if len(processing_times) > 200:\n",
    "                    processing_times.popleft()\n",
    "\n",
    "                _, f_width = frame.shape[:2]\n",
    "                # Mean processing time [ms].\n",
    "                processing_time = np.mean(processing_times) * 1000\n",
    "                fps = 1000 / processing_time\n",
    "                cv2.putText(\n",
    "                    img=frame,\n",
    "                    text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                    org=(20, 40),\n",
    "                    fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    fontScale=f_width / 1000,\n",
    "                    color=(0, 0, 255),\n",
    "                    thickness=1,\n",
    "                    lineType=cv2.LINE_AA,\n",
    "                )\n",
    "                \n",
    "                video_frames.append(frame)\n",
    "\n",
    "                # Use this workaround if there is flickering.\n",
    "                if use_popup:\n",
    "                    cv2.imshow(winname=title, mat=frame)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    # escape = 27\n",
    "                    if key == 27:\n",
    "                        break\n",
    "                else:\n",
    "                    # Encode numpy array to jpg.\n",
    "                    _, encoded_img = cv2.imencode(\n",
    "                        ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "                    )\n",
    "                    # Create an IPython image.\n",
    "                    i = display.Image(data=encoded_img)\n",
    "                    # Display the image in this notebook.\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(i)\n",
    "                    \n",
    "            elif type_ == FrameType.Depth:\n",
    "                t1 = time.time()\n",
    "                if t1 - t0 > 1:\n",
    "                    frame_depth = frame_.to_array().astype(float)\n",
    "                    cv2.resize(src=frame_depth, dsize=(640, 480), interpolation=cv2.INTER_AREA, dst=frame_depth)\n",
    "                    # list of distances for each mask\n",
    "                    dist = get_distance_to_humans(frame_depth, masks)\n",
    "                    del frame_depth\n",
    "                    t0 = time.time()\n",
    "\n",
    "                \n",
    "\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    \n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if device is not None:\n",
    "            # Stop capturing.\n",
    "            device.stop()\n",
    "            device.close()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()\n",
    "        return video_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "### Run Live Object Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] [Freenect2Impl] enumerating devices...\n",
      "[Info] [Freenect2Impl] 9 usb devices connected\n",
      "[Info] [Freenect2Impl] found valid Kinect v2 @2:4 with serial 017163350647\n",
      "[Info] [Freenect2Impl] found 1 devices\n",
      "[Info] [Freenect2DeviceImpl] opening...\n",
      "[Info] [Freenect2DeviceImpl] transfer pool sizes rgb: 20*16384 ir: 60*8*33792\n",
      "[Info] [Freenect2DeviceImpl] opened\n",
      "[Info] [Freenect2DeviceImpl] starting...\n",
      "[Info] [Freenect2DeviceImpl] submitting rgb transfers...\n",
      "[Info] [Freenect2DeviceImpl] submitting depth transfers...\n",
      "[Info] [Freenect2DeviceImpl] started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QApplication: invalid style override 'adwaita' passed, ignoring it.\n",
      "\tAvailable styles: Windows, Fusion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] [DepthPacketStreamParser] 2 packets were lost\n",
      "[Info] [Freenect2DeviceImpl] stopping...\n",
      "[Info] [Freenect2DeviceImpl] canceling rgb transfers...\n",
      "[Info] [Freenect2DeviceImpl] canceling depth transfers...\n",
      "[Info] [Freenect2DeviceImpl] stopped\n",
      "[Info] [Freenect2DeviceImpl] closing...\n",
      "[Info] [Freenect2DeviceImpl] releasing usb interfaces...\n",
      "[Info] [Freenect2DeviceImpl] deallocating usb transfer pools...\n",
      "[Info] [Freenect2DeviceImpl] closing usb device...\n",
      "[Info] [Freenect2DeviceImpl] closed\n"
     ]
    }
   ],
   "source": [
    "video_frames = run_object_detection(source=0, flip=True, use_popup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('test_kinect_test_depth_every_sec.mp4', fourcc, 5, (video_frames[0].shape[1], video_frames[0].shape[0]))\n",
    "for i in range(len(video_frames)):\n",
    "    out.write(video_frames[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Object Detection on a Video File\n",
    "\n",
    "If you do not have a webcam, you can still run this demo with a video file. Any [format supported by OpenCV](https://docs.opencv.org/4.5.1/dd/d43/tutorial_py_video_display.html) will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "test_replace": {
     "use_popup=False": "use_popup=False, skip_first_frames=280"
    }
   },
   "outputs": [],
   "source": [
    "video_file = \"../data/video/Coco Walking in Berkeley.mp4\"\n",
    "\n",
    "run_object_detection(source=video_file, flip=False, use_popup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "1. [SSDLite MobileNetV2](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/ssdlite_mobilenet_v2)\n",
    "2. [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)\n",
    "3. [Non-Maximum Suppression](https://paperswithcode.com/method/non-maximum-suppression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
